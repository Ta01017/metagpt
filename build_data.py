# fixed_dataset_builder.py
import scipy.io
import numpy as np
import pickle
import re
import os
from pathlib import Path
import glob
from structure_lang.tokenizer import StructureTokenizerExtended

class FixedSiAlphaDatasetBuilder:
    def __init__(self, target_spec_dim: int = 128):
        self.target_spec_dim = target_spec_dim
        self.tokenizer = StructureTokenizerExtended()
        self.material = "Si-Alpha"
        
        print(f"âœ… ä½¿ç”¨æ‰©å±•çš„tokenizerï¼Œæ”¯æŒæ›´å¹¿çš„å‚æ•°èŒƒå›´")
        print(f"  ææ–™: {self.material}")
        print(f"  è¯è¡¨å¤§å°: {self.tokenizer.vocab_size}")
    
    def parse_filename(self, filename: str) -> dict:
        """ä»æ–‡ä»¶åè§£æç»“æ„å‚æ•°"""
        pattern = r'T_P([\d\.e+-]+)_D([\d\.e+-]+)_H([\d\.e+-]+)_num-idx(\d+)\.mat'
        match = re.match(pattern, filename)
        
        if match:
            P, D, H, idx = match.groups()
            return {
                'P': float(P),
                'D': float(D), 
                'H': float(H),
                'idx': int(idx)
            }
        else:
            raise ValueError(f"æ— æ³•è§£ææ–‡ä»¶å: {filename}")
    
    def process_spectrum(self, spectrum: np.ndarray) -> np.ndarray:
        """å¤„ç†å…‰è°±æ•°æ®"""
        original_dim = spectrum.shape[0]
        
        if original_dim == self.target_spec_dim:
            interpolated = spectrum
        else:
            original_x = np.linspace(0, 1, original_dim)
            target_x = np.linspace(0, 1, self.target_spec_dim)
            interpolated = np.interp(target_x, original_x, spectrum)
        
        # å½’ä¸€åŒ–
        spec_min, spec_max = interpolated.min(), interpolated.max()
        if spec_max - spec_min > 1e-10:
            normalized = (interpolated - spec_min) / (spec_max - spec_min)
        else:
            normalized = np.zeros_like(interpolated)
        
        return normalized.astype(np.float32)
    
    def parameters_to_tokens(self, P: float, D: float, H: float) -> list:
        """å°†ç»“æ„å‚æ•°è½¬æ¢ä¸ºtokenåºåˆ—"""
        # å•ä½è½¬æ¢ï¼šç±³ â†’ çº³ç±³
        P_nm = P * 1e9
        H_nm = H * 1e9
        R_nm = (D / 2) * 1e9  # ç›´å¾„è½¬åŠå¾„
        
        # é‡åŒ–åˆ°æœ€æ¥è¿‘çš„ç¦»æ•£å€¼
        P_quantized = self._quantize_to_nearest(P_nm, self.tokenizer.P_vals)
        H_quantized = self._quantize_to_nearest(H_nm, self.tokenizer.H_vals)
        R_quantized = self._quantize_to_nearest(R_nm, self.tokenizer.R_vals)
        
        # æ„å»ºtokenåºåˆ—
        tokens = [
            f"PX_{P_quantized}",
            f"PY_{P_quantized}", 
            "SUB_Glass_Substrate",
            f"L1_MAT_{self.material}",
            "L1_SHAPE_CYL",
            f"L1_H_{H_quantized}",
            f"L1_R_{R_quantized}"
        ]
        
        token_ids = self.tokenizer.encode(tokens)
        
        # éªŒè¯é‡åŒ–è¯¯å·®ï¼ˆé™ä½è­¦å‘Šé˜ˆå€¼åˆ°20%ï¼‰
        p_error = abs(P_nm - P_quantized) / P_nm * 100 if P_nm > 0 else 0
        r_error = abs(R_nm - R_quantized) / R_nm * 100 if R_nm > 0 else 0
        h_error = abs(H_nm - H_quantized) / H_nm * 100 if H_nm > 0 else 0
        
        if max(p_error, r_error, h_error) > 20:  # è¯¯å·®å¤§äº20%æ—¶è­¦å‘Š
            print(f"âš ï¸  è¾ƒå¤§é‡åŒ–è¯¯å·®: P={p_error:.1f}%, R={r_error:.1f}%, H={h_error:.1f}%")
        
        return token_ids
    
    def _quantize_to_nearest(self, value: float, allowed_values: list) -> int:
        """å°†è¿ç»­å€¼é‡åŒ–åˆ°æœ€æ¥è¿‘çš„ç¦»æ•£å€¼"""
        return min(allowed_values, key=lambda x: abs(x - value))
    
    def validate_parameters(self, P: float, D: float, H: float) -> bool:
        """éªŒè¯å‚æ•°æ˜¯å¦åœ¨æ‰©å±•åçš„èŒƒå›´å†…"""
        P_nm = P * 1e9
        H_nm = H * 1e9
        R_nm = (D / 2) * 1e9
        
        P_valid = min(self.tokenizer.P_vals) <= P_nm <= max(self.tokenizer.P_vals)
        H_valid = min(self.tokenizer.H_vals) <= H_nm <= max(self.tokenizer.H_vals)
        R_valid = min(self.tokenizer.R_vals) <= R_nm <= max(self.tokenizer.R_vals)
        
        return P_valid and H_valid and R_valid
    
   def build_dataset(self, folder_path: str, output_prefix: str, max_samples: int = None):
        """æ„å»ºæ•°æ®é›†"""
        mat_files = glob.glob(os.path.join(folder_path, "*.mat"))
        
        if max_samples:
            mat_files = mat_files[:max_samples]
            
        print(f"\nğŸ“ å¤„ç†æ–‡ä»¶å¤¹: {folder_path}")
        print(f"ğŸ“Š æ‰¾åˆ° {len(mat_files)} ä¸ªMATæ–‡ä»¶")
        
        all_spectra = []
        all_tokens = []
        failed_files = []
        
        for i, file_path in enumerate(mat_files):
            if i % 1000 == 0 and i > 0:
                print(f"è¿›åº¦: {i}/{len(mat_files)}")
            
            try:
                filename = Path(file_path).name
                params = self.parse_filename(filename)
                
                # éªŒè¯å‚æ•°èŒƒå›´
                if not self.validate_parameters(params['P'], params['D'], params['H']):
                    # è¯¦ç»†é”™è¯¯ä¿¡æ¯
                    P_nm = params['P'] * 1e9
                    R_nm = (params['D'] / 2) * 1e9
                    H_nm = params['H'] * 1e9
                    
                    error_msg = f"å‚æ•°è¶…å‡ºèŒƒå›´: P={P_nm:.1f}nm, R={R_nm:.1f}nm, H={H_nm:.1f}nm"
                    failed_files.append((filename, error_msg))
                    continue
                
                # åŠ è½½å’Œå¤„ç†æ•°æ®
                mat_data = scipy.io.loadmat(file_path)
                spectrum = mat_data['T'].flatten()

                processed_spectrum = self.process_spectrum(spectrum)
                tokens = self.parameters_to_tokens(params['P'], params['D'], params['H'])
                
                all_spectra.append(processed_spectrum)
                all_tokens.append(tokens)
                
            except Exception as e:
                failed_files.append((filename, str(e)))
                continue
        
        # ä¿å­˜æ•°æ®
        spectra_array = np.array(all_spectra, dtype=np.float32)
        np.save(f"{output_prefix}_spec.npy", spectra_array)
        with open(f"{output_prefix}_struct.pkl", 'wb') as f:
            pickle.dump(all_tokens, f)
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\nâœ… å¤„ç†å®Œæˆ:")
        print(f"   æˆåŠŸ: {len(all_spectra)} ä¸ªæ ·æœ¬")
        print(f"   å¤±è´¥: {len(failed_files)} ä¸ªæ ·æœ¬")
        
        if failed_files:
            print(f"\nâŒ å¤±è´¥æ ·æœ¬ç¤ºä¾‹:")
            for filename, error in failed_files[:5]:
                print(f"   {filename}: {error}")
        
        return spectra_array, all_tokens, failed_files, len(mat_files)  # è¿”å›å¤±è´¥ä¿¡æ¯å’Œæ–‡ä»¶æ€»æ•°
# ä½¿ç”¨ä¿®å¤åçš„æ„å»ºå™¨
def build_fixed_datasets(train_folder: str, val_folder: str, output_dir: str = ".", 
                        spec_dim: int = 128, test_mode: bool = False):
    """ä½¿ç”¨ä¿®å¤åçš„æ„å»ºå™¨æ„å»ºæ•°æ®é›†"""
    
    os.makedirs(output_dir, exist_ok=True)
    builder = FixedSiAlphaDatasetBuilder(target_spec_dim=spec_dim)
    
    max_samples = 100 if test_mode else None
    
    print("=" * 60)
    print("ğŸš€ å¼€å§‹æ„å»ºä¿®å¤åçš„æ•°æ®é›†")
    print("=" * 60)
    
    # å¤„ç†è®­ç»ƒé›†
    train_spec, train_struct, train_failed, train_total = builder.build_dataset(
        train_folder, 
        os.path.join(output_dir, "spec_train"),
        max_samples=max_samples
    )
    
    # å¤„ç†éªŒè¯é›†
    val_spec, val_struct, val_failed, val_total = builder.build_dataset(
        val_folder, 
        os.path.join(output_dir, "spec_val"),
        max_samples=max_samples
    )
    
    print(f"\nğŸ‰ æ•°æ®é›†æ„å»ºæˆåŠŸ!")
    print(f"   è®­ç»ƒé›†: {train_spec.shape[0]:,} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {val_spec.shape[0]:,} æ ·æœ¬")
    print(f"   è®­ç»ƒé›†å¤±è´¥ç‡: {len(train_failed)/train_total*100:.1f}%")
    print(f"   éªŒè¯é›†å¤±è´¥ç‡: {len(val_failed)/val_total*100:.1f}%")
    
    return train_spec, train_struct, val_spec, val_struct

# ä¿®å¤tokenizerçš„å‚æ•°ç¦»æ•£å€¼è®¾ç½®
def create_better_tokenizer():
    """åˆ›å»ºæ›´åˆç†çš„ç¦»æ•£å€¼è®¾ç½®"""
    
    # åŸºäºå®é™…æ•°æ®åˆ†æï¼Œåˆ›å»ºæ›´å¯†é›†çš„ç¦»æ•£å€¼
    print("ğŸ”§ åˆ›å»ºæ›´åˆç†çš„ç¦»æ•£å€¼è®¾ç½®...")
    
    # æ›´å¯†é›†çš„På€¼ï¼ˆå‘¨æœŸï¼‰ï¼šä»50nmåˆ°1000nmï¼Œæ­¥é•¿10nm
    P_vals = list(range(50, 1001, 10))
    
    # æ›´å¯†é›†çš„Rå€¼ï¼ˆåŠå¾„ï¼‰ï¼šä»20nmåˆ°500nmï¼Œæ­¥é•¿5nm  
    R_vals = list(range(20, 501, 5))
    
    # æ›´å¯†é›†çš„Hå€¼ï¼ˆé«˜åº¦ï¼‰ï¼šä»50nmåˆ°1200nmï¼Œæ­¥é•¿10nm
    H_vals = list(range(50, 1201, 10))
    
    print(f"  æ”¹è¿›çš„ç¦»æ•£å€¼è®¾ç½®:")
    print(f"  P_vals: {len(P_vals)}ä¸ªå€¼, {P_vals[0]}nm - {P_vals[-1]}nm")
    print(f"  R_vals: {len(R_vals)}ä¸ªå€¼, {R_vals[0]}nm - {R_vals[-1]}nm")
    print(f"  H_vals: {len(H_vals)}ä¸ªå€¼, {H_vals[0]}nm - {H_vals[-1]}nm")
    
    return P_vals, R_vals, H_vals

# æ”¹è¿›çš„tokenizeræ‰©å±•ç±»
class ImprovedStructureTokenizerExtended(StructureTokenizerExtended):
    """æ”¹è¿›çš„tokenizerï¼Œä½¿ç”¨æ›´åˆç†çš„ç¦»æ•£å€¼"""


     
    def __init__(self):
        super().__init__()
        
        # ä½¿ç”¨æ”¹è¿›çš„ç¦»æ•£å€¼
        self.P_vals, self.R_vals, self.H_vals = create_better_tokenizer()
        
        # é‡æ–°æ„å»ºè¯è¡¨
        self._rebuild_vocab_with_improved_ranges()
    
    def _rebuild_vocab_with_improved_ranges(self):
        """ä½¿ç”¨æ”¹è¿›çš„ç¦»æ•£å€¼é‡æ–°æ„å»ºè¯è¡¨"""
        self.vocab = {}
        self.inv_vocab = {}
        idx = 0
        
        # ç‰¹æ®Štoken
        for t in self.special_tokens:
            self.vocab[t] = idx; idx += 1

        # PX, PY (ä½¿ç”¨æ”¹è¿›çš„P_vals)
        for P in self.P_vals:
            self.vocab[f"PX_{P}"] = idx; idx += 1
            self.vocab[f"PY_{P}"] = idx; idx += 1

        # substrate
        self.vocab["SUB_Glass_Substrate"] = idx; idx += 1

        # materials
        self.materials = ["SiO2", "TiO2", "Si-Alpha"]
        for m in self.materials:
            self.vocab[f"L1_MAT_{m}"] = idx; idx += 1

        # shapes
        shapes = ["CYL", "RECT"]
        for sh in shapes:
            self.vocab[f"L1_SHAPE_{sh}"] = idx; idx += 1


        # height (ä½¿ç”¨æ”¹è¿›çš„H_vals)
        for H in self.H_vals:
            self.vocab[f"L1_H_{H}"] = idx; idx += 1

        # CYL radius (ä½¿ç”¨æ”¹è¿›çš„R_vals)
        for R in self.R_vals:
            self.vocab[f"L1_R_{R}"] = idx; idx += 1

        # RECT width/length (ä½¿ç”¨ä¸Rç›¸åŒçš„èŒƒå›´)
        for W in self.R_vals:  # é‡ç”¨R_valsçš„èŒƒå›´
            self.vocab[f"L1_W_{W}"] = idx; idx += 1
            self.vocab[f"L1_L_{W}"] = idx; idx += 1

        # CoT tokens
        self.cot_tokens = ["[COT]"]
        self.cot_tokens += [f"COT_MAT_{m}" for m in self.materials]
        self.cot_tokens += ["COT_SHAPE_CYL", "COT_SHAPE_RECT"]
        for t in self.cot_tokens:
            if t not in self.vocab:
                self.vocab[t] = idx; idx += 1

        self.inv_vocab = {v:k for k,v in self.voca